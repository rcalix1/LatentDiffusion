{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181de94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import urllib\n",
    "import numpy as np\n",
    "import PIL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e767a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6675b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiffusionModel:\n",
    "def __init__(self, start_schedule=0.0001, end_schedule=0.02, timesteps =␣\n",
    ",→300):\n",
    "self.start_schedule = start_schedule self.end_schedule = end_schedule self.timesteps = timesteps\n",
    "\"\"\" if\n",
    "            betas = [0.1, 0.2, 0.3, ...]\n",
    "        then\n",
    "            alphas = [0.9, 0.8, 0.7, ...]\n",
    "            alphas_cumprod = [0.9, 0.9 * 0.8, 0.9 * 0.8, * 0.7, ...]\n",
    "\"\"\"\n",
    "        self.betas = torch.linspace(start_schedule, end_schedule, timesteps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "def forward(self, x_0, t, device): \"\"\"\n",
    "        x_0: (B, C, H, W)\n",
    "        t: (B,)\n",
    "        \"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "sqrt_alphas_cumprod_t = self.get_index_from_list(self.alphas_cumprod. ,→sqrt(), t, x_0.shape)\n",
    "sqrt_one_minus_alphas_cumprod_t = self.get_index_from_list(torch.sqrt(1. ,→ - self.alphas_cumprod), t, x_0.shape)\n",
    "       mean = sqrt_alphas_cumprod_t.to(device) * x_0.to(device)\n",
    "       variance = sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device)\n",
    "return mean + variance, noise.to(device)\n",
    "@torch.no_grad()\n",
    "def backward(self, x, t, model, **kwargs):\n",
    "\"\"\"\n",
    "Calls the model to predict the noise in the image and returns\n",
    "the denoised image.\n",
    "Applies noise to this image, if we are not in the last step yet.\n",
    "\"\"\"\n",
    "betas_t = self.get_index_from_list(self.betas, t, x.shape) sqrt_one_minus_alphas_cumprod_t = self.get_index_from_list(torch.sqrt(1.\n",
    ",→ - self.alphas_cumprod), t, x.shape)\n",
    "sqrt_recip_alphas_t = self.get_index_from_list(torch.sqrt(1.0 / self.\n",
    ",→alphas), t, x.shape)\n",
    "mean = sqrt_recip_alphas_t * (x - betas_t * model(x, t, **kwargs) /␣\n",
    ",→sqrt_one_minus_alphas_cumprod_t) posterior_variance_t = betas_t\n",
    "if t == 0: return mean\n",
    "else:\n",
    "noise = torch.randn_like(x)\n",
    "variance = torch.sqrt(posterior_variance_t) * noise return mean + variance\n",
    "   @staticmethod\n",
    "def get_index_from_list(values, t, x_shape): batch_size = t.shape[0]\n",
    "\"\"\"\n",
    "pick the values from vals\n",
    "according to the indices stored in `t` \"\"\"\n",
    "result = values.gather(-1, t.cpu()) \"\"\"\n",
    "       if\n",
    "       x_shape = (5, 3, 64, 64)\n",
    "           -> len(x_shape) = 4\n",
    "           -> len(x_shape) - 1 = 3\n",
    "           and thus we reshape `out` to dims\n",
    "       (batch_size, 1, 1, 1)\n",
    "\"\"\"\n",
    "return result.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t. ,→device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3cce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SHAPE = (32, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4071e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.Resize(IMAGE_SHAPE), # Resize the input image transforms.ToTensor(), # Convert to torch tensor (scales data into [0,1]) transforms.Lambda(lambda t: (t * 2) - 1), # Scale data between [-1, 1]\n",
    "])\n",
    "reverse_transform = transforms.Compose([\n",
    "transforms.Lambda(lambda t: (t + 1) / 2), # Scale data between [0,1] transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC transforms.Lambda(lambda t: t * 255.), # Scale data between [0.,255.] transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)), # Convert␣\n",
    ",→into an uint8 numpy array transforms.ToPILImage(), # Convert to PIL image\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b780ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module): def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "def forward(self, time):\n",
    "device = time.device\n",
    "half_dim = self.dim // 2\n",
    "embeddings = math.log(10000) / (half_dim - 1)\n",
    "embeddings = torch.exp(torch.arange(half_dim, device=device) *␣\n",
    ",→-embeddings)\n",
    "embeddings = time[:, None] * embeddings[None, :]\n",
    "embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1) return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Block(nn.Module):\n",
    "def __init__(self, channels_in, channels_out, time_embedding_dims, labels,␣\n",
    ",→num_filters = 3, downsample=True): super().__init__()\n",
    "    self.time_embedding_dims = time_embedding_dims\n",
    "self.time_embedding = SinusoidalPositionEmbeddings(time_embedding_dims) self.labels = labels\n",
    "if labels:\n",
    "           self.label_mlp = nn.Linear(1, channels_out)\n",
    "       self.downsample = downsample\n",
    "if downsample:\n",
    "self.conv1 = nn.Conv2d(channels_in, channels_out, num_filters,␣\n",
    ",→padding=1)\n",
    "self.final = nn.Conv2d(channels_out, channels_out, 4, 2, 1)\n",
    "else:\n",
    "self.conv1 = nn.Conv2d(2 * channels_in, channels_out, num_filters,␣\n",
    ",→padding=1)\n",
    "self.final = nn.ConvTranspose2d(channels_out, channels_out, 4, 2, 1)\n",
    "       self.bnorm1 = nn.BatchNorm2d(channels_out)\n",
    "       self.bnorm2 = nn.BatchNorm2d(channels_out)\n",
    "       self.conv2 = nn.Conv2d(channels_out, channels_out, 3, padding=1)\n",
    "       self.time_mlp = nn.Linear(time_embedding_dims, channels_out)\n",
    "       self.relu = nn.ReLU()\n",
    "def forward(self, x, t, **kwargs):\n",
    "o = self.bnorm1(self.relu(self.conv1(x)))\n",
    "o_time = self.relu(self.time_mlp(self.time_embedding(t))) o = o + o_time[(..., ) + (None, ) * 2]\n",
    "if self.labels:\n",
    "label = kwargs.get('labels')\n",
    "o_label = self.relu(self.label_mlp(label)) o = o + o_label[(..., ) + (None, ) * 2]\n",
    "o = self.bnorm2(self.relu(self.conv2(o))) return self.final(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6cad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet(nn.Module):\n",
    "def __init__(self, img_channels = 3, time_embedding_dims = 128, labels =␣\n",
    ",→False, sequence_channels = (64, 128, 256, 512, 1024)): super().__init__()\n",
    "        self.time_embedding_dims = time_embedding_dims\n",
    "        sequence_channels_rev = reversed(sequence_channels)\n",
    "        self.downsampling = nn.ModuleList([Block(channels_in, channels_out,␣ ,→time_embedding_dims, labels) for channels_in, channels_out in␣ ,→zip(sequence_channels, sequence_channels[1:])])\n",
    "self.upsampling = nn.ModuleList([Block(channels_in, channels_out,␣ ,→time_embedding_dims, labels,downsample=False) for channels_in, channels_out␣ ,→in zip(sequence_channels[::-1], sequence_channels[::-1][1:])])\n",
    "       self.conv1 = nn.Conv2d(img_channels, sequence_channels[0], 3, padding=1)\n",
    "       self.conv2 = nn.Conv2d(sequence_channels[0], img_channels, 1)\n",
    "def forward(self, x, t, **kwargs): residuals = []\n",
    "o = self.conv1(x)\n",
    "for ds in self.downsampling:\n",
    "           o = ds(o, t, **kwargs)\n",
    "residuals.append(o)\n",
    "for us, res in zip(self.upsampling, reversed(residuals)):\n",
    "o = us(torch.cat((o, res), dim=1), t, **kwargs) return self.conv2(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 256\n",
    "NO_EPOCHS = 100\n",
    "PRINT_FREQUENCY = 10\n",
    "LR = 0.001\n",
    "VERBOSE = False\n",
    "unet = UNet(labels=True)\n",
    "unet.to(device)\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c4b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,␣ ,→download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,␣ ,→shuffle=True, num_workers=8, drop_last=True)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,␣ ,→download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,␣ ,→shuffle=False, num_workers=8, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb696f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diffusion_model = DiffusionModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023fb32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(NO_EPOCHS): mean_epoch_loss = [] mean_epoch_loss_val = []\n",
    "for batch, label in trainloader:\n",
    "t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,)).long(). ,→to(device)\n",
    "        batch = batch.to(device)\n",
    "        batch_noisy, noise = diffusion_model.forward(batch, t, device)\n",
    "        predicted_noise = unet(batch_noisy, t, labels = label.reshape(-1,1).\n",
    ",→float().to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.nn.functional.mse_loss(noise, predicted_noise)\n",
    "        mean_epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "for batch, label in testloader:\n",
    "t = torch.randint(0, diffusion_model.timesteps, (BATCH_SIZE,)).long(). ,→to(device)\n",
    "        batch = batch.to(device)\n",
    "        batch_noisy, noise = diffusion_model.forward(batch, t, device)\n",
    "predicted_noise = unet(batch_noisy, t, labels = label.reshape(-1,1). ,→float().to(device))\n",
    "        loss = torch.nn.functional.mse_loss(noise, predicted_noise)\n",
    "        mean_epoch_loss_val.append(loss.item())\n",
    "if epoch % PRINT_FREQUENCY == 0: print('---')\n",
    "print(f\"Epoch: {epoch} | Train Loss {np.mean(mean_epoch_loss)} | Val␣ ,→Loss {np.mean(mean_epoch_loss_val)}\")\n",
    "if VERBOSE:\n",
    "with torch.no_grad():\n",
    "                plot_noise_prediction(noise[0], predicted_noise[0])\n",
    "                plot_noise_distribution(noise, predicted_noise)\n",
    "torch.save(unet.state_dict(), f\"epoch: {epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unet = UNet(labels=True) \n",
    "unet.load_state_dict(torch.load((\"epoch: 90\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8096ccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc826f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLASSES = len(classes)\n",
    "NUM_DISPLAY_IMAGES = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(16)\n",
    "plt.figure(figsize=(15,15))\n",
    "f, ax = plt.subplots(NUM_CLASSES, NUM_DISPLAY_IMAGES, figsize = (100,100)) for c in range(NUM_CLASSES):\n",
    "    imgs = torch.randn((NUM_DISPLAY_IMAGES, 3) + IMAGE_SHAPE).to(device) for i in reversed(range(diffusion_model.timesteps)):\n",
    "        t = torch.full((1,), i, dtype=torch.long, device=device)\n",
    "labels = torch.tensor([c] * NUM_DISPLAY_IMAGES). ,→resize(NUM_DISPLAY_IMAGES, 1).float().to(device)\n",
    "imgs = diffusion_model.backward(x=imgs, t=t, model=unet.eval(). ,→to(device), labels = labels)\n",
    "for idx, img in enumerate(imgs): ax[c][idx].imshow(reverse_transform(img)) ax[c][idx].set_title(f\"Class: {classes[c]}\", fontsize = 100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d78b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the class you want to generate (e.g., \"bird\")\n",
    "specific_class = 'cat'\n",
    "class_index = classes.index(specific_class) # Get the index of the specific␣\n",
    ",→class\n",
    "NUM_DISPLAY_IMAGES = 5 # Number of images to generate\n",
    "torch.manual_seed(78) # For reproducibility plt.figure(figsize=(15, 15))\n",
    "# Initialize random noise images\n",
    "imgs = torch.randn((NUM_DISPLAY_IMAGES, 3) + IMAGE_SHAPE).to(device)\n",
    "# Reverse the diffusion process for the specified class\n",
    "for i in reversed(range(diffusion_model.timesteps)):\n",
    "t = torch.full((1,), i, dtype=torch.long, device=device) # Current timestep labels = torch.tensor([class_index] * NUM_DISPLAY_IMAGES).\n",
    ",→resize(NUM_DISPLAY_IMAGES, 1).float().to(device)\n",
    "imgs = diffusion_model.backward(x=imgs, t=t, model=unet.eval().to(device),␣\n",
    ",→labels=labels)\n",
    "# Plot the generated images\n",
    "plt.figure(figsize=(15, 15))\n",
    "for idx, img in enumerate(imgs):\n",
    "plt.subplot(1, NUM_DISPLAY_IMAGES, idx + 1) plt.imshow(reverse_transform(img)) plt.title(f\"Class: {specific_class}\", fontsize=20) plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b6b698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f296e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078ab01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14608a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9075bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616a12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ef19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49145b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b4b7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af81fc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e58216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6751f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b18767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc1422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
